{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Qualify1.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPPT631Seu31JxoQ52oaKwU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VirtualGoat/Twitter-Data-Mining/blob/master/Qualify1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-LIpBOyffSC",
        "colab_type": "code",
        "outputId": "c932bf22-67f3-4db5-a09b-448280df8b24",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        }
      },
      "source": [
        "!pip install GetOldTweets3\n",
        "#Using GOT module as twitter's official API restrict the tweets to a specific number thus preventing the user to download old tweets(> 2 months)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting GetOldTweets3\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/f4/a00c2a7c90801abc875325bb5416ce9090ac86d06a00cc887131bd73ba45/GetOldTweets3-0.0.11-py3-none-any.whl\n",
            "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from GetOldTweets3) (4.2.6)\n",
            "Collecting pyquery>=1.2.10\n",
            "  Downloading https://files.pythonhosted.org/packages/78/43/95d42e386c61cb639d1a0b94f0c0b9f0b7d6b981ad3c043a836c8b5bc68b/pyquery-1.4.1-py2.py3-none-any.whl\n",
            "Collecting cssselect>0.7.9\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/d4/3b5c17f00cce85b9a1e6f91096e1cc8e8ede2e1be8e96b87ce1ed09e92c5/cssselect-1.1.0-py2.py3-none-any.whl\n",
            "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
            "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNLbc-LhhBF6",
        "colab_type": "code",
        "outputId": "8eb2f10a-e613-48fd-cca2-89ef262124f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        }
      },
      "source": [
        "#Accessing the data that has been stored. \n",
        "from google.colab import drive\n",
        "import pickle\n",
        "import GetOldTweets3 as got\n",
        "drive.mount('/content/drive')\n",
        "DATA_PATH = \"/content/drive/My Drive/Colab Notebooks/Internship/Tweet Data/New/Mumbai\"\n",
        "users4=open(DATA_PATH+'/activeuserlist.pickle','rb')\n",
        "real_tweets3=pickle.load(users4)\n",
        "users4.close()\n",
        "print(len(real_tweets3))\n",
        "lisi=list()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "3422\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_czYL2wChCYe",
        "colab_type": "code",
        "outputId": "b868f294-ea4c-4ad0-e126-c33ddd684d86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import multiprocessing \n",
        "#Using multiprocessing as it utilizes all the cores of CPU and enables parallel computing. \n",
        "crawled=set() #unique elements can only be stored. \n",
        "m = multiprocessing.Manager()\n",
        "q = m.Queue()\n",
        "for i in real_tweets3[301:600]:      \n",
        "    q.put(i)\n",
        "print(q.qsize())\n",
        "li=dict()\n",
        "import datetime\n",
        "'''\n",
        "Steps for checking: \n",
        "--> The usernames have been put in a queue. \n",
        "--> Each process will access the usernames and pull the tweet data fo the usernames. \n",
        "--> To prevent multiple processes from accessing the same username, te username will be popped once a process accepts the username.\n",
        "'''\n",
        "from time import sleep\n",
        "def check_if_rich(q):  \n",
        "  keywords=['singapore','america','spain','germany','france','paris','berlin','europe','bali','indonesia','switzerland','australia','malaysia','united kingdom','london','mauritius','maldives','thailand','dubai','netherland','tokyo','japan','canada','new york','california','fransisco','angeles','vegas','miami','florida','usa','bermuda','iceland','denmark','luxembourg','kuwait','hong kong','greece','mercedes','bmw','lexus','jaguar','royce','fortuner','audi','bentley','porsche','ferrari','volvo','triumph','enfield','interceptor','rover','cadillac','maserati','lamborghini','hilfiger','rolex','rado','tissot','giordano','diesel','fossil','guess','kors','calvin klein','heuer','armani','adidas','nike','piguet','blanc','hublot','patek', 'bandra', 'khar', 'mahalaxmi', 'south bombay', 'worli', 'vashi', 'colaba','dadar','malabar','lokhandwala','andheri','powai','juhu','peddar','altamount','parel', 'trident','marriot','itc','oberoi','taj','hyatt','lalit','sahara','four seasons','westin','orchid','kohinoor','fariyas','marine plaza','meluha','rodas','radisson','shalimar','waterstones','ambassador','ramada','mirage']\n",
        "  while(q.empty()!=True): #Repeat until queue contains some elements.    \n",
        "    try:\n",
        "        uname=q.get()     #Queue does not contain unique usernames so a separate set is used to store the crawled usernames \n",
        "        print(uname)\n",
        "        if uname not in crawled:\n",
        "            print(datetime.datetime.now())\n",
        "            count=0\n",
        "            crawled.add(uname)\n",
        "            tweetCriteria = got.manager.TweetCriteria().setUsername(uname).setSince(\"2019-01-01\").setUntil(\"2020-03-10\") #Pull yearly tweets\n",
        "            tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
        "            print(\"Crawling User: \",uname)      \n",
        "            for j in tweets:\n",
        "                if any(word in ((j.text).lower()).split() for word in keywords):  #Checking for the presence of qualifying keywords in tweets.  \n",
        "                    print(j.text)\n",
        "                    count=count+1\n",
        "                    li[uname]=count\n",
        "                    print(li)\n",
        "            print(\"Going for next user.\")\n",
        "            sleep(30)\n",
        "    except:\n",
        "      print(\"sleeping...\")\n",
        "      sleep(300)\n",
        "      continue\n",
        "  return li\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "299\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QVYKw51MhDy3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from concurrent.futures import ProcessPoolExecutor\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "# with concurrent.futures.ProcessPoolExecutor(7) as executor:\n",
        "#     future = executor.submit(check_if_rich, q)\n",
        "#     return_value = future.result()\n",
        "\n",
        "\n",
        "\n",
        "pool = ProcessPoolExecutor(4)\n",
        " \n",
        "future = pool.submit(check_if_rich, q)\n",
        "result=future.result()\n",
        "\n",
        "users5=open(DATA_PATH+'/qualified301to600.pickle','wb')\n",
        "real_tweets4=pickle.dump(result,users5)\n",
        "users5.close()\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}